# -*- coding: utf-8 -*-
"""churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FVk_wcobr9osZqaH9GbWmz0h70fzxsOt

### PREDICTING CHURN

## 0. PROBLEM FORMULATION


*   Understand which factors influence churn the most
*   Identify individuals with high probability of churning


*   Build a churn prediction model
"""

#pip install streamlit

import streamlit as st

# Set page configuration
st.set_page_config(
    page_title="Customer Churn Prediction",
    page_icon="üè¶",
    layout="wide"
)

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import pickle
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from PIL import Image
import xgboost as xgb
import pydot
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV
import pickle
from xgboost import XGBClassifier
from sklearn.preprocessing import MinMaxScaler



"""## 1. LOADING THE DATA"""

#data_source = https://www.kaggle.com/datasets/saurabhbadole/bank-customer-churn-prediction-dataset
raw_file = pd.read_csv("Churn_Modelling.csv", encoding="latin1")
raw_file.head()

"""## 2. DATA PRE-PROCESSING

2.1. Understanding the data
"""

#Investigating all the element within each feature

for column in raw_file:
    unique_vals = np.unique(raw_file[column].fillna('0'))
    nr_values = len(unique_vals)
    if nr_values <= 12:
        print("The number of values for features {} :{} -- {}".format(column, nr_values, unique_vals))
    else:
        print("The number of values for features {} :{}".format(column, nr_values))

raw_file.isnull().sum()

"""2.2. Visualizing the data"""

#Count plot of our Y - check the balance of our dataset
plt.figure(figsize=(8,6))
sns.countplot(data=raw_file, x="Exited")
plt.title("Count Plot of Exited", fontsize=16)
plt.xlabel("Exited", fontsize=14)
plt.ylabel("count", fontsize=14)
plt.xticks([0,1], labels=["Not Exited", "Exited"])
plt.show()

#Limiting the data
raw_file_viz = raw_file[["CreditScore", "Geography", "Gender", "Age", "Tenure", "Balance", "NumOfProducts", "HasCrCard", "IsActiveMember", "EstimatedSalary", "Exited"]]

#Visualizing
g = sns.pairplot(raw_file_viz, hue = "Exited")

#Investigate all the featurs by our y

features = ["Geography", "Gender", "Age", "Tenure", "NumOfProducts", "HasCrCard", "IsActiveMember"]

for f in features:
    plt.figure()
    ax = sns.countplot(x=f, data=raw_file_viz, hue = "Exited", palette="Set1")

#Investigating the distribution of all Numerical Values

#Identifying all numeric columns
numerics = ["int16", "int32", "int64", "float16", "float32", "float64"]
n_variables = raw_file_viz.select_dtypes(include=numerics).columns

#Increase the size of the sns plots
sns.set(rc={"figure.figsize":(8,5)})

for c in n_variables:
    x = raw_file_viz[c].values
    ax = sns.boxplot(x, color = "#D1EC46")
    print("The meadian is: ", raw_file_viz[c].median())
    plt.title(c)
    plt.show()

"""2.3. Preparing the final Data"""

#Making categorical variables into numeric representation
new_raw_data = pd.get_dummies(raw_file_viz, columns = ["Geography", "Gender", "HasCrCard", "IsActiveMember"])

#Scaling our columns
scale_vars = ["CreditScore", "EstimatedSalary", "Tenure", "Balance", "Age", "NumOfProducts"]
scaler = MinMaxScaler()
new_raw_data[scale_vars] = scaler.fit_transform(new_raw_data[scale_vars])
new_raw_data.head()

"""## 3. RUNNING XGBOOST"""

#Features (X) and (y)
X = new_raw_data.drop(columns=["Exited"])
y = new_raw_data["Exited"]

#Splitting the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the XGBoost Classifier
model = XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)
model.fit(X_train, y_train)

#Predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

#Training and Testing Accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}")

#Feature Importance
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": model.feature_importances_
}).sort_values(by="Importance", ascending=False)

#Plotting feature importance
plt.figure(figsize=(10,6))
plt.barh(feature_importance["Feature"], feature_importance["Importance"], align="center")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance")
plt.gca().invert_yaxis() #Invert y-axis for better readability
plt.show()

#Displaying feature importance
print("\nFeature Importance")
feature_importance

#Confusion Matrix function
def plot_confusion_matrix(cm, classes=None, title="Confusion Matrix"):
    """Plot a confusion matrix."""
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0, vmax=1, annot=True, cmap="Blues",fmt=".2f")
    plt.title(title)
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.show()

#Calculate confusion matrix
cm = confusion_matrix(y_test, y_test_pred, normalize="true")
plot_confusion_matrix(cm, classes=["Not Existed", "Existed"])

"""We are not predicting exited well....."""

#Re-Running XGBoost using SMOTE
#Applying SMOTE to oversample the minority class in the training set
smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#Initialize the XGBoost classifier
model = XGBClassifier(use_label_encoder= False, eval_metric="logloss", random_state=42)
model.fit(x_train_resampled, y_train_resampled)

#Predictions
y_train_pred = model.predict(x_train_resampled)
y_test_pred = model.predict(X_test)

#Training and Testing Accuracy
train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}")

#PLotting feature Importance
feature_importance = model.feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(X_train.columns, feature_importance, align="center")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance")
plt.gca().invert_yaxis()
plt.show()

#Calculate confusion matrix
cm = confusion_matrix(y_test, y_test_pred, normalize="true")
plot_confusion_matrix(cm, classes=["Not Existed", "Existed"])

#Export the first tree in the ensemble to DOT format
#dot_data = xgb.to_graphviz(model, num_trees=0)

#Save and display the tree
output_file = "xgboost_tree"
#dot_data.render(output_file, format="png", view=False) #SAves "xgboost_tree.png"

#Display the tree image using PIL (for inline visualization in Jupyter or Python)
img = Image.open(f"{output_file}.png")
plt.figure(figsize=(30, 30))
plt.imshow(img)
plt.axis("off")
plt.show()

#Tree visual is not easy to read so we will train a simple DT to visual the tree
#Train a Decision Tree Classifier
dt = DecisionTreeClassifier(random_state=42, max_depth=3)
dt.fit(x_train_resampled, y_train_resampled)

#Predictions and evaluation
y_train_pred = dt.predict(x_train_resampled)
y_test_pred = dt.predict(X_test)

train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}")

#Import the decision tree to DOT format
dot_data = export_graphviz(
    dt,
    out_file=None,
    feature_names=new_raw_data.drop("Exited", axis=1).columns,
    class_names=["Not Existed", "Existed"],
    filled=True,
    rounded=True,
    special_characters=True
)

#Visualize the tree using Graphviz
graph = graphviz.Source(dot_data)
graph

"""##4. Hyperparameter Tuning"""

#Define the parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 200, 300], #Number of trees
    'max_depth': [3,5,7,10], #Maximum depth of a tree
    'learning_rate': [0.01, 0.05, 0.1, 0.2], #Step size shrinkage
    'subsample': [0.6, 0.8, 1.0], #Fraction of samples to grow trees
    'colsample_bytree': [0.6, 0.8, 1.0], #Fraction of features for the tree building
    'gamma': [0, 1, 5], #Minimum loss reduction for split
    'reg_lambda': [1, 10, 50], #L2 regularization term
}

#Initialize the base XGBoost Classifier
xgb_base = XGBClassifier(use_label_encoder=False, eval_metric="auc", random_state=42)

#Apply RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_grid,
    n_iter=50,  #Number of parameter settings sampled
    scoring='f1',
    cv=3,  #3-fold cross validation
    random_state=42,
    verbose=3,
    n_jobs=-1,  #Use all available cores
)

#Fit the RandomizedSearchCV
random_search.fit(x_train_resampled, y_train_resampled)

#Get the best parameters and model
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

#Re-train the model with the best hyperparameters
best_model = random_search.best_estimator_
best_model.fit(x_train_resampled, y_train_resampled)

#Predictions
y_train_pred = best_model.predict(x_train_resampled)
y_test_pred = best_model.predict(X_test)
all_df_predict = best_model.predict(X)
all_df_predict_prob = best_model.predict_proba(X)

#Training and Testing Accuracy
train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}")

#Feature Importance
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": best_model.feature_importances_
}).sort_values(by="Importance", ascending=False)
feature_importance
#metrics: https://sclkit.learn.org/1.5/modules/model_evaluation.htmlescoring-parameter

#Calculate confusion matrix
cm = confusion_matrix(y_test, y_test_pred, normalize="true")
plot_confusion_matrix(cm, classes=["Not Existed", "Existed"])

"""##5. Storing Models and Results"""

#Adding the predictions back to the original dataset
new_raw_data["Exited Prediction"] = all_df_predict
new_raw_data["Exited Prediction Probability"] = all_df_predict_prob[:,1]

#Exporting all the data with prediction
new_raw_data.to_excel("bank_churn_data.xlsx")

#Storing the Feature Importance
feature_importance["Feature Importance Score"] = feature_importance["Importance"].round(4)
feature_importance.to_excel("feature_importance.xlsx")

#Save the best model to a file.

with open("best_model.pkl", "wb") as file:
    pickle.dump(best_model, file)

print("Model saved successfully!")

#Saving the scaler
with open("scaler.pkl", "wb") as file:
    pickle.dump(scaler, file)

print("Scaler saved successfully!")


"""##6. Streamlit App"""


# Title and description
st.title("üè¶ Bank Customer Churn Prediction")
st.markdown("""
This app predicts whether a bank customer is likely to churn based on their profile information.
Adjust the parameters in the sidebar and click **Predict** to see the results.
""")

# Load the model and scaler
@st.cache_resource
def load_models():
    try:
        with open("best_model.pkl", "rb") as file:
            model = pickle.load(file)
        with open("scaler.pkl", "rb") as file:
            scaler = pickle.load(file)
        return model, scaler
    except FileNotFoundError as e:
        st.error(f"Model files not found: {e}")
        st.stop()

model, scaler = load_models()

# Define feature names that match your trained model
# Based on your preprocessing, these should be the features:
feature_names = [
    'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 
    'EstimatedSalary', 'Geography_France', 'Geography_Germany', 
    'Geography_Spain', 'Gender_Female', 'Gender_Male', 
    'HasCrCard_0', 'HasCrCard_1', 'IsActiveMember_0', 'IsActiveMember_1'
]

# Create sidebar for user inputs
st.sidebar.header("üìä Customer Information")

# Collect user inputs with more user-friendly labels
user_inputs = {}

# Numerical inputs
user_inputs['CreditScore'] = st.sidebar.slider("Credit Score", 300, 850, 600)
user_inputs['Age'] = st.sidebar.slider("Age", 18, 100, 30)
user_inputs['Tenure'] = st.sidebar.slider("Tenure (years with bank)", 0, 10, 2)
user_inputs['Balance'] = st.sidebar.number_input("Account Balance", 0.0, 300000.0, 8000.0, 100.0)
user_inputs['NumOfProducts'] = st.sidebar.slider("Number of Products", 1, 4, 2)
user_inputs['EstimatedSalary'] = st.sidebar.number_input("Estimated Salary", 0.0, 300000.0, 60000.0, 1000.0)

# Categorical inputs
st.sidebar.subheader("Customer Details")

# Geography - using radio buttons for better UX
geo = st.sidebar.radio("Geography", ["France", "Germany", "Spain"])
user_inputs['Geography_France'] = 1 if geo == "France" else 0
user_inputs['Geography_Germany'] = 1 if geo == "Germany" else 0
user_inputs['Geography_Spain'] = 1 if geo == "Spain" else 0

# Gender
gender = st.sidebar.radio("Gender", ["Female", "Male"])
user_inputs['Gender_Female'] = 1 if gender == "Female" else 0
user_inputs['Gender_Male'] = 1 if gender == "Male" else 0

# Has Credit Card
has_card = st.sidebar.radio("Has Credit Card?", ["Yes", "No"])
user_inputs['HasCrCard_0'] = 1 if has_card == "No" else 0
user_inputs['HasCrCard_1'] = 1 if has_card == "Yes" else 0

# Is Active Member
active_member = st.sidebar.radio("Is Active Member?", ["Yes", "No"])
user_inputs['IsActiveMember_0'] = 1 if active_member == "No" else 0
user_inputs['IsActiveMember_1'] = 1 if active_member == "Yes" else 0

# Convert inputs to DataFrame
input_df = pd.DataFrame([user_inputs])

# Ensure the columns are in the correct order
input_df = input_df[feature_names]

# Scale numerical features
scale_vars = ["CreditScore", "EstimatedSalary", "Tenure", "Balance", "Age", "NumOfProducts"]
input_df_scaled = input_df.copy()
input_df_scaled[scale_vars] = scaler.transform(input_df[scale_vars])



# Create two columns for layout
col1, col2 = st.columns(2)

with col1:
    st.header("üìà Prediction")
    
    if st.button("üîÆ Predict Churn", type="primary", use_container_width=True):
        try:
            # Make prediction
            prediction = model.predict(input_df_scaled)[0]
            prediction_proba = model.predict_proba(input_df_scaled)[0]
            
            # Display results
            if prediction == 1:
                st.error(f"üö® High Risk of Churning!")
                st.metric("Churn Probability", f"{prediction_proba[1]*100:.1f}%")
            else:
                st.success(f"‚úÖ Low Risk of Churning")
                st.metric("Retention Probability", f"{prediction_proba[0]*100:.1f}%")
            
            # Show probability breakdown
            st.subheader("Probability Breakdown")
            prob_df = pd.DataFrame({
                'Status': ['Retain', 'Churn'],
                'Probability': [prediction_proba[0], prediction_proba[1]]
            })
            
            fig = px.bar(prob_df, x='Status', y='Probability', 
                        color='Status',
                        color_discrete_map={'Retain': 'green', 'Churn': 'red'},
                        text_auto='.1%')
            fig.update_layout(yaxis_tickformat='.0%')
            st.plotly_chart(fig, use_container_width=True)
            
        except Exception as e:
            st.error(f"Error making prediction: {e}")

with col2:
    st.header("‚öñÔ∏è Feature Importance")
    
    try:
        # Load feature importance from Excel
        feature_importance_df = pd.read_excel("feature_importance.xlsx")
        
        # Plot feature importance
        fig = px.bar(feature_importance_df.sort_values('Feature Importance Score', ascending=True),
                     x='Feature Importance Score', 
                     y='Feature',
                     orientation='h',
                     title="Top Features Influencing Churn",
                     color='Feature Importance Score',
                     color_continuous_scale='RdYlGn_r')
        
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        
    except FileNotFoundError:
        st.info("Feature importance data not available. Please run the model training script first.")

# --- NEW SECTION: DECISION LOGIC VISUALIZATION FIX ---
st.markdown("---") # Visual separator
st.header("üå≥ Decision Logic Visualization")
try:
    # This replaces the crashing graphviz logic with a stable image display
    st.image("xgboost_tree.png", 
             caption="XGBoost Decision Tree - Model Logic Flow", 
             use_container_width=True)
except FileNotFoundError:
    st.warning("Tree image not found. Please save 'xgboost_tree.png' to the project folder.")


# Display the input values for transparency
st.sidebar.header("üìã Input Summary")
st.sidebar.dataframe(input_df.T.rename(columns={0: "Value"}), use_container_width=True)

# Footer
st.sidebar.markdown("---")
st.sidebar.info(
    """
    **Instructions:**
    1. Adjust customer parameters in the sidebar
    2. Click **Predict Churn** button
    3. View prediction results and feature importance
    """
)